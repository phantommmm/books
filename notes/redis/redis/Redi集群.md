## Redis集群（Cluster）
描述:Redis 3.0引入

集群中的节点分为主节点和从节点：只有主节点负责读写请求和集群信息的维护；从节点只进行主节点数据和状态信息的复制。

    集群的作用：
    1、数据分区：数据分区(或称数据分片)是集群最核心的功能。
    
    集群将数据分散到多个节点，一方面突破了Redis单机内存大小的限制，存储容量大大增加；另一方面每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力。
    
    Redis单机内存大小受限问题，在介绍持久化和主从复制时都有提及；例如，如果单机内存太大，bgsave和bgrewriteaof的fork操作可能导致主进程阻塞，主从环境下主机切换时可能导致从节点长时间无法提供服务，全量复制阶段主节点的复制缓冲区可能溢出……。
    
    2、高可用：集群支持主从复制和主节点的自动故障转移（与哨兵类似）；当任一节点发生故障时，集群仍然可以对外提供服务。
### 集群的搭建
#### 1.Redis命令搭建
##### （1）启动节点(相互独立)
    #redis-7000.conf
    port 7000
    cluster-enabled yes//是否开启集群 yes集群模式(cluster) no 单机模式（standalone）
    
    cluster-config-file "node-7000.conf"//集群配置文件
        每当集群信息发生变化时（如增减节点），集群内所有节点会将最新信息更新到该配置文件
        当Redis节点以集群模式启动时，会首先寻找是否有集群配置文件，如果有则使用文件中的配置启动，如果没有，则初始化配置并将配置保存到文件中。
    
    logfile "log-7000.log"
    dbfilename "dump-7000.rdb"
    daemonize yes
    
    命令cluster nodes查看节点信息，第一项为节点ID由40个16进制字符串组成，只会在集群初始化的时候创建一次。
##### （2）节点握手（连成网络）
cluster meet ip port:集群里加入了节点，加入后，各个节点互相知道
##### （3）分配槽
    1.集群有16384个槽，槽是数据管理和迁移的基本单位。当数据库中的16384个槽都分配了节点时，集群处于上线状态（ok）；如果有任意一个槽没有分配节点，则集群处于下线状态（fail）。
    cluster info命令，查看集群信息
    
    cluster addslots 分配槽
    redis-cli -p 7000 cluster addslots {0..5461}
    redis-cli -p 7001 cluster addslots {5462..10922}
    redis-cli -p 7002 cluster addslots {10923..16383}
##### （4）指定主从关系
    cluster replicate 节点ID
    redis-cli -p 8000 cluster replicate be816eba968bc16c884b963d768c945e86ac51ae
    redis-cli -p 8001 cluster replicate 788b361563acb175ce8232569347812a12f1fdb4
    redis-cli -p 8002 cluster replicate a26f1624a3da3e5197dde267de683d61bb2dcbf1
#### 数据分区方案
##### 哈希分区：对数据的特征值（如key）进行哈希，然后根据哈希值决定数据落在哪个节点。

        判断数据分区的2个重要标准：
            (1)数据分布是否均匀
            (2)增加或删减节点对数据分布的影响。
        由于哈希的随机性，哈希分区基本可以保证数据分布均匀；因此在比较哈希分区方案时，重点要看增减节点对数据分布的影响。
##### 哈希取余分区
        计算key的hash值，然后对节点数量进行取余，从而决定数据映射到哪个节点上。
            问题：当新增或删减节点时，节点数量发生变化，系统中所有的数据都需要重新计算映射关系，引发大规模数据迁移。
##### 一致性哈希分区
        将整个哈希值空间组织成一个虚拟的圆环，如下图所示，范围为0-2^32-1；
        对于每个数据，根据key计算hash值，确定数据在环上的位置，然后从此位置沿环顺时针行走，找到的第一台服务器（集群节点）就是其应该映射到的服务器。
            问题：当节点数量较少时，增加或删减节点，对单个节点的影响可能很大，造成数据的严重不平衡。
            例：如果去掉node2，node4中的数据由总数据的1/4左右变为1/2左右，与其他节点相比负载过高。
##### 带虚拟节点的一致性哈希分区（Redis集群使用）
        1.引入了槽作为虚拟节点
        2.每个实际节点包含一定数量的槽，每个槽包含哈希值在一定范围内的数据。
        3.数据的映射关系由 数据hash->实际节点，变成了 数据hash->槽->实际节点。
        4.当增加或删除节点时，只需重新分配槽，数据仍然均衡
        5.Redis 槽数量为16384
### 节点通信机制
#### 两个端口
    哨兵：节点分为数据节点、哨兵节点，前者存储数据，后者实现额外的控制功能
    集群：只有单个节点，既有存储数据，也参与集群的维护，但每个节点提供2个tcp端口
    
    普通端口：即我们在前面指定的端口(7000等)。普通端口主要用于为客户端提供服务（与单机节点类似）；但在节点间数据迁移时也会使用。
    集群端口：端口号是普通端口+10000（10000是固定值，无法改变），如7000节点的集群端口为17000。
            集群端口只用于节点之间的通信，如搭建集群、增减节点、故障转移等操作时节点间的通信；不要使用客户端连接集群接口。为了保证集群可以正常工作，在配置防火墙时，要同时开启普通端口和集群端口。
#### 通信协议
##### 广播
指向集群内所有节点发送消息；

优点是集群的收敛速度快(集群收敛是指集群内所有节点获得的集群信息是一致的)，缺点是每条消息都要发送给所有节点，CPU、带宽等消耗较大。
##### Gossip协议
在节点数量有限的网络中，每个节点都“随机”的与部分节点通信（并不是真正的随机，而是根据特定的规则选择通信的节点），经过一番杂乱无章的通信，每个节点的状态很快会达到一致。

Gossip协议的优点有负载(比广播)低、去中心化、容错性高(因为通信有冗余)等；缺点主要是集群的收敛速度慢。
#### 消息发送
集群中的节点采用固定频率（每秒10次）的定时任务进行通信相关的工作：判断是否需要发送消息及消息类型、确定接收节点、发送消息等。
如果集群状态发生了变化，如增减节点、槽状态变更，通过节点间的通信，所有节点会很快得知整个集群的状态，使集群收敛。

        MEET消息：在节点握手阶段，当节点收到客户端的CLUSTER MEET命令时，会向新加入的节点发送MEET消息，请求新节点加入到当前集群；新节点收到MEET消息后会回复一个PONG消息。
    
        PING消息：集群里每个节点每秒钟会选择部分节点发送PING消息，接收者收到消息后会回复一个PONG消息。
        PING消息的内容是自身节点和部分其他节点的状态信息；作用是彼此交换信息，以及检测节点是否在线。
        PING消息使用Gossip协议发送，接收节点的选择兼顾了收敛速度和带宽成本，具体规则如下：
            (1)随机找5个节点，在其中选择最久没有通信的1个节点
            (2)扫描节点列表，选择最近一次收到PONG消息时间大于cluster_node_timeout/2的所有节点，防止这些节点长时间未更新。
    
        PONG消息：PONG消息封装了自身状态数据。可以分为两种：
            第一种是在接到MEET/PING消息后回复的PONG消息；
            第二种是指节点向集群广播PONG消息，这样其他节点可以获知该节点的最新信息，例如故障恢复后新的主节点会广播PONG消息。
    
        FAIL消息：当一个主节点判断另一个主节点进入FAIL状态时，会向集群广播这一FAIL消息；接收节点会将这一FAIL消息保存起来，便于后续的判断。
    
        PUBLISH消息：节点收到PUBLISH命令后，会先执行该命令，然后向集群广播这一消息，接收节点也会执行该PUBLISH命令。
### 集群命令的实现
每个节点维护clusterNode(节点状态)和clusterState（集群状态）
##### cluster meet 实现过程
        假设A节点发送cluster meet命令，将B节点加入到A所在的集群，则执行的操作如下：
    
        1)  A为B创建一个clusterNode结构，并将其添加到clusterState的nodes字典中
    
        2)  A向B发送MEET消息
    
        3)  B收到MEET消息后，会为A创建一个clusterNode结构，并将其添加到clusterState的nodes字典中
    
        4)  B回复A一个PONG消息
    
        5)  A收到B的PONG消息后，便知道B已经成功接收自己的MEET消息
    
        6)  然后，A向B返回一个PING消息
    
        7)  B收到A的PING消息后，便知道A已经成功接收自己的PONG消息，握手完成
    
        8)  之后，A通过Gossip协议将B的信息广播给集群内其他节点，其他节点也会与B握手；一段时间后，集群收敛，B成为集群内的一个普通节点
    
        通过上述过程可以发现，集群中两个节点的握手过程与TCP类似，都是三次握手：A向B发送MEET；B向A发送PONG；A向B发送PING。
##### cluster addslots
集群中槽的分配信息，存储在clusterNode的slots数组(占用16384/8 个字节，16384个比特；每个比特对应一个槽：比特值为1，则该比特对应的槽在节点中；比特值为0，则该比特对应的槽不在节点中)

和clusterState的slots(数组的每个元素都是一个指向clusterNode结构的指针；如果槽还没有分配给任何节点，则为NULL)数组中，二者的区别在于：前者存储的是该节点中分配了哪些槽，后者存储的是集群中所有槽分别分布在哪个节点。

        cluster addslots命令接收一个槽或多个槽作为参数，例如在A节点上执行cluster addslots {0..10}命令，是将编号为0-10的槽分配给A节点，具体执行过程如下：
    
        1)  遍历输入槽，检查它们是否都没有分配，如果有一个槽已分配，命令执行失败；方法是检查输入槽在clusterState.slots[]中对应的值是否为NULL。
    
        2)  遍历输入槽，将其分配给节点A；方法是修改clusterNode.slots[]中对应的比特为1，以及clusterState.slots[]中对应的指针指向A节点
    
        3)  A节点执行完成后，通过节点通信机制通知其他节点，所有节点都会知道0-10的槽分配给了A节点
### 客户端访问集群
#### redis-cli
           1.当节点收到redis-cli发来的命令(如set/get)时，过程如下：
            1）计算key属于哪个槽：CRC16(key) & 16383
                127.0.0.1:7000>cluster keyslot key1
                (Integer) 9189
            2）判断key所在的槽是否在当前节点：
                假设key位于第i个槽，clusterState.slots[i]则指向了槽所在的节点，如果clusterState.slots[i]==clusterState.myself，说明槽在当前节点，可以直接在当前节点执行命令；
                否则，说明槽不在当前节点，则查询槽所在节点的地址(clusterState.slots[i].ip/port)，并将其包装到MOVED错误中返回给redis-cli。
            3）redis-cli收到MOVED错误后，根据返回的ip和port重新发送请求。
                redis -cli  -c -p 7000//-c 指定集群模式，如果不指定则不会处理MOVED错误
#### smart客户端(java程序JedisCluster)
        public static void test() {
           Set<HostAndPort> nodes = new HashSet<>();
           nodes.add(new HostAndPort("192.168.72.128", 7000));
           nodes.add(new HostAndPort("192.168.72.128", 7001));
           nodes.add(new HostAndPort("192.168.72.128", 7002));
           nodes.add(new HostAndPort("192.168.72.128", 8000));
           nodes.add(new HostAndPort("192.168.72.128", 8001));
           nodes.add(new HostAndPort("192.168.72.128", 8002));
           JedisCluster cluster = new JedisCluster(nodes);
           System.out.println(cluster.get("key1"));
           cluster.close();
        }
    
            （1）JedisCluster初始化时，在内部维护slot->node的缓存，方法是连接任一节点，执行cluster slots命令
            （2）此外，JedisCluster为每个节点创建连接池(即JedisPool)。
            （3）当执行命令时，JedisCluster根据key->slot->node选择需要连接的节点，发送命令。
            如果成功，则命令执行完毕。如果执行失败，则会随机选择其他节点进行重试，并在出现MOVED错误时，使用cluster slots重新同步slot->node的映射关系。
    
            总结：
        （1）JedisCluster中已经包含所有节点的连接池，因此JedisCluster要使用单例。
    
        （2）客户端维护了slot->node映射关系以及为每个节点创建了连接池，当节点数量较多时，应注意客户端内存资源和连接资源的消耗。
    
        （3）Jedis较新版本针对JedisCluster做了一些性能方面的优化，如cluster slots缓存更新和锁阻塞等方面的优化，应尽量使用2.8.2及以上版本的Jedis。
### 实际操作
#### 1. 集群伸缩
伸缩的核心是槽迁移：修改槽与节点的对应关系，实现槽(即数据)在节点之间的移动。
##### 增加节点
        假设要增加7003和8003节点，其中8003是7003的从节点；步骤如下：
    
    （1）启动节点：方法参见集群搭建
    
    （2）节点握手：可以使用cluster meet命令，但在生产环境中建议使用redis-trib.rb的add-node工具，其原理也是cluster meet，但它会先检查新节点是否已加入其它集群或者存在数据，避免加入到集群后带来混乱。
    
    1   redis-trib.rb add-node 192.168.72.128:7003 192.168.72.128 7000
    2   redis-trib.rb add-node 192.168.72.128:8003 192.168.72.128 7000


    （3）迁移槽：推荐使用redis-trib.rb的reshard工具实现。reshard自动化程度很高，只需要输入redis-trib.rb reshard ip:port (ip和port可以是集群中的任一节点)，然后按照提示输入以下信息，槽迁移会自动完成：
    
    待迁移的槽数量：16384个槽均分给4个节点，每个节点4096个槽，因此待迁移槽数量为4096
    目标节点id：7003节点的id
    源节点的id：7000/7001/7002节点的id
    
    （4）指定主从关系：方法参见集群搭建
##### 减少节点
        假设要下线7000/8000节点，可以分为两步：
    
        （1）迁移槽：使用reshard将7000节点中的槽均匀迁移到7001/7002/7003节点
    
        （2）下线节点：使用redis-trib.rb del-node工具；应先下线从节点再下线主节点，因为若主节点先下线，从节点会被指向其他主节点，造成不必要的全量复制。
    
        redis-trib.rb del-node 192.168.72.128:7001 {节点8000的id}
        redis-trib.rb del-node 192.168.72.128:7001 {节点7000的id}
##### ASK错误：非源数据库的源节点正在迁移槽时
    客户端收到ASK错误后，从中读取目标节点的地址信息，并向目标节点重新发送请求，就像收到MOVED错误时一样。
    但是二者有很大区别：
                        ASK错误说明数据正在迁移，不知道何时迁移完成，因此重定向是临时的，SMART客户端不会刷新slots缓存；
                        MOVED错误重定向则是(相对)永久的，SMART客户端会刷新slots缓存。
##### 故障判定
        1：集群中每个节点都会定期向其他节点发出ping命令，如果没有收到回复，就认为该节点为疑似下线（主观下线），然后在集群中传播该信息
        2：当集群中的某个节点，收到半数以上认为某节点已下线的信息，就会真的标记该节点为已下线（客观下线），并在集群中传播该信息
        3：如果已下线的节点是master节点，那就意味着一部分插槽无法写入了
        4：如果集群任意master挂掉，且当前master没有slave，集群进入fail状态
        5：如果集群超过半数以上master挂掉，无论是否有slave，集群进入fail状态
        6：当集群不可用时，所有对集群的操作做都不可用，收到CLUSTERDOWN The cluster is down错误信息
##### 故障恢复
        发现某个master下线后，集群会进行故障恢复操作，来将一个slave变成master，基于Raft算法，大致步骤如下：
        1：某个slave向集群中每个节点发送请求，要求选举自己为master
    
        2：如果收到请求的节点没有选举过其他slave，会同意
    
        3：当集群中有超过节点数一半的节点同意该slave的请求，则该Slave选举成功
    
        4：如果有多个slave同时参选，可能会出现没有任何slave当选的情况，将会等待一个随机时间，再次发出选举请求
    
        5：选举成功后，slave会通过 slaveof no one命令把自己变成master
        如果故障后还想集群继续工作，可设置cluster-require-full-coverage为no，默认yes
    
        对于集群故障恢复的说明：
        1：master挂掉了，重启还可以加入集群，但挂掉的slave重启，如果对应的master变化了，是不能加入集群的，除非修改它们的配置文件，将其master指向新master
        2：只要主从关系建立，就会触发主和该从采用save方式持久化数据，不论你是否禁止save
        3：在集群中，如果默认主从关系的主挂了并立即重启，如果主没有做持久化，数据会完全丢失，从而从的数据也被清空


#### 2.故障转移
        节点数量：在故障转移阶段，需要由主节点投票选出哪个从节点成为新的主节点；
                从节点选举胜出需要的票数为N/2+1；其中N为主节点数量(包括故障主节点)，但故障主节点实际上不能投票。
                因此为了能够在故障发生时顺利选出从节点，集群中至少需要3个主节点(且部署在不同的物理机上)。
    
        故障转移时间：从主节点故障发生到完成转移，所需要的时间主要消耗在主观下线识别、主观下线传播、选举延迟等几个环节；具体时间与参数cluster-node-timeout有关，一般来说：
    
        故障转移时间(毫秒) ≤ 1.5 * cluster-node-timeout + 1000
    
        cluster-node-timeout的默认值为15000ms(15s)，因此故障转移时间会在20s量级。
#### 3. 集群的限制及应对方法
            （1）key批量操作受限：例如mget、mset操作，只有当操作的key都位于一个槽时，才能进行。
            针对该问题，一种思路是在客户端记录槽与key的信息，每次针对特定槽执行mget/mset；
                        另外一种思路是使用Hash Tag，将在下一小节介绍。
    
            （2）keys/flushall等操作：keys/flushall等操作可以在任一节点执行，但是结果只针对当前节点，例如keys操作只返回当前节点的所有键。
                针对该问题，可以在客户端使用cluster nodes获取所有节点信息，并对其中的所有主节点执行keys/flushall等操作。
    
            （3）事务/Lua脚本：集群支持事务及Lua脚本，但前提条件是所涉及的key必须在同一个节点。Hash Tag可以解决该问题。
    
            （4）数据库：单机Redis节点可以支持16个数据库，集群模式下只支持一个，即db0。
    
            （5）复制结构：只支持一层复制结构，不支持嵌套。
#### 4. Hash Tag
        Hash Tag原理是：当一个key包含 {} 的时候，不对整个key做hash，而仅对 {} 包括的字符串做hash。
    
        Hash Tag可以让不同的key拥有相同的hash值，从而分配在同一个槽里；这样针对不同key的批量操作(mget/mset等)，以及事务、Lua脚本等都可以支持。
        不过Hash Tag可能会带来数据分配不均的问题，这时需要：
                        (1)调整不同节点中槽的数量，使数据分布尽量均匀；
                        (2)避免对热点数据使用Hash Tag，导致请求分布不均。
           例子：23.png
#### 5.参数优化
##### cluster_node_timeout (默认为15s)
        （1）影响PING消息接收节点的选择：值越大对延迟容忍度越高，选择的接收节点越少，可以降低带宽，但会降低收敛速度；应根据带宽情况和应用要求进行调整。
    
        （2）影响故障转移的判定和时间：值越大，越不容易误判，但完成转移消耗时间越长；应根据网络状况和应用要求进行调整。
##### cluster-require-full-coverage
        前面提到，只有当16384个槽全部分配完毕时，集群才能上线。这样做是为了保证集群的完整性，但同时也带来了新的问题：
        当主节点发生故障而故障转移尚未完成，原主节点中的槽不在任何节点中，此时会集群处于下线状态，无法响应客户端的请求。
    
        cluster-require-full-coverage参数可以改变这一设定：如果设置为no，则当槽没有完全分配时，集群仍可以上线。
        参数默认值为yes，如果应用对可用性要求较高，可以修改为no，但需要自己保证槽全部分配。
