#### **吞吐量**

一个系统的吞吐量（承压能力）与request对CPU的消耗、外部接口、IO等等紧密关联。单个request 对CPU消耗越高，外部系统接口、IO速度越慢，系统吞吐能力越低，反之越高。

系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间

​    QPS（TPS）：（Query Per Second）每秒钟request/事务 数量

​    并发数： 系统同时处理的request/事务数

​    响应时间： 一般取平均响应时间

QPS（TPS）= 并发数/平均响应时间   一秒内可以处理的请求数量

**QPS**：Queries Per Second意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。

**TPS：**是TransactionsPerSecond的缩写，也就是事务数/秒。它是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。

高并发总结：https://blog.csdn.net/qq_42629110/article/details/84979482

基于Redisson的分布式锁：

https://blog.csdn.net/liuxiao723846/article/details/88131065

https://zhuanlan.zhihu.com/p/99187446

https://blog.csdn.net/u010391342/article/details/84372342

即时通讯系统：http://www.52im.net/thread-812-1-1.html

#### 动静态资源分离

动态资源放在tomcat上，静态资源放在nginx上，用户第一次访问时，加载静态资源于用户电脑本地上，后面加载时直接从本地拿。

**http浏览器缓存**

浏览器第一次请求服务器，此时浏览器肯定没有缓存，则直接调用服务器端，服务器在返回的信息的信息头中添加 **ETag和Last-Modified**参数信息，返回给客户端浏览器缓存。

然后浏览器以后的请求，先判断是否有缓存，那么怎么判断有缓存呢，有三步：

（1）是否过期  

（2）对ETag信息对比 ，ETag：  就是一个生成的字符串

（3）对Last-Modified信息对比；Last-Modified：服务端最后一次修改的时间。

如果没有过期，则将信息直接返回回去，如果过期了，则将请求发送到服务端，此时request的头信息中带着ETag和Last-Modified信息，responose头信息中也带着这两个参数，如果一致，则表示要访问的资源没有发生改变，直接返回304，如果不一致，则表明资源改变，会请求服务端，返回200。

#### nginx负载均衡

配置2个tomcat服务器，将nginx秒杀请求分别转发至2个服务器。

#### java后端限流

使用Google guava的RateLimiter来进行限流
例如：每秒钟只允许10个人进入秒杀步骤. (可能是拦截掉90%的用户请求，拦截后直接返回"很遗憾，没抢到")

```
/**
 * 秒杀前的限流.
 * 使用了Google guava的RateLimiter
 */
@Service
public class AccessLimitServiceImpl implements AccessLimitService {
    /**
     * 每秒钟只发出10个令牌，拿到令牌的请求才可以进入秒杀过程
     */
    private RateLimiter seckillRateLimiter = RateLimiter.create(10);

    /**
     * 尝试获取令牌
     * @return
     */
    @Override
    public boolean tryAcquireSeckill() {
        return seckillRateLimiter.tryAcquire();
    }
}
```

#### nginx漏桶限流 

```
limit_req_zone用来限制单位时间内的请求数，即速率限制,
limit_req_conn用来限制同一时间连接数，即并发限制。

//限制连接速率
http {
    limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
    key:限流对象(binary_remote_addr) remote_addr（客户端IP 对该IP限流）binary(压缩内存占用量)
    zone:定义共享内存区存储访问信息。one:10m(名为one的10m内存区)
    rate:1r/s表示每秒最多处理1个请求，nginx是以毫秒为单位处理的，所以1r/s实际上是每1000ms处理一个请求
    
    server {
        location /search/ {
            limit_req zone=one burst=5 nodelay;
            zone:设置哪个区域做限制，与上面对应
            burst:当大量请求爆发时，超过了访问限制的请求可以先放到这个缓冲区队列
            nodelay:立即处理，表明5个请求立即处理。但是，当5个请求处理完毕后，后续请求不会立即得到处理，缓冲队列会一个个的释放坑，只能按照1000ms一次释放坑，即实现速率稳定。
        }
}        
//限制并发速率
limit_conn_zone $binary_remote_addr zone=addr:10m;
limit_conn_zone $server_name zone=server:10m;

server {
    location /download/ {
        limit_conn addr 20;//对该缓冲区限制同时只能有20个连接 限制单个ip并发数
        limit_conn server 100;//限制server并发数100
    }
```



#### **redis数据结构**

http://zhangtielei.com/posts/blog-redis-dict.html

#### Redission

![img](https://img2018.cnblogs.com/blog/905646/201909/905646-20190916141147653-1082477092.png)

##### 源码中加锁Lua代码

**为什么要用lua语言**

因为复杂的业务逻辑可以封装在Lua脚本中发送给redis,保证执行的原子性

```
//判断KEYS[1]锁是否存在，不存在则加锁
if (redis.call('exists', KEYS[1]) == 0) then 
//锁名 客户端id 次数
//hset myLock 8743c9c0-0795-4907-87fd-6c719a6b4586:1 1
        redis.call('hset', KEYS[1], ARGV[2], 1);
        //设置锁存在时间 
         redis.call('pexpire', KEYS[1], ARGV[1]); 
         return nil;
          end;
if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then
        redis.call('hincrby', KEYS[1], ARGV[2], 1);
        redis.call('pexpire', KEYS[1], ARGV[1]); 
        return nil;
        end;
return redis.call('pttl', KEYS[1]);
```

KEYS[1]:表示你加锁的那个key，比如说
RLock lock = redisson.getLock(“myLock”);这里你自己设置了加锁的那个锁key就是“myLock”。
ARGV[1]:表示锁的有效期，默认30s，（没有watch dog下到期后自动释放锁，防止服务器奔溃导致死锁）
ARGV[2]:表示表示加锁的客户端ID,类似于下面这样：8743c9c0-0795-4907-87fd-6c719a6b4586:1

##### 锁互斥机制

如果在这个时候，另一个客户端(客户端2)来尝试加锁，执行了同样的一段lua脚本，会怎样呢？

第一个if判断会执行“exists myLock”，发现myLock这个锁key已经存在了。

接着第二个if判断会执行“hexists mylock 客户端id”，来判断myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。

所以，客户端2会获取到pttl myLock返回的一个数字，这个数字代表了myLock这个锁key的剩余生存时间。

比如还剩15000毫秒的生存时间。此时客户端2会进入一个while循环，不停的尝试加锁。

##### 可重入锁机制

```
#重入加锁
RLock lock = redisson.getLock("myLock")
lock.lock();//加锁
//业务代码
lock.lock();//加锁
//业务代码
lock.unlock();
lock.unlock();

incrby myLock 8743c9c0-0795-4907-87fd-6c71a6b4586:1  1
myLock:
    {
        8743c9c0-0795-4907-87fd-6c719a6b4586:1  2
    }
```

##### 释放锁机制

```
if (redis.call('exists', KEYS[1]) == 0) then
       redis.call('publish', KEYS[2], ARGV[1]);
        return 1; 
        end;
if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then 
     return nil;
     end;
local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); 
if (counter > 0) then
     redis.call('pexpire', KEYS[1], ARGV[2]); 
     return 0; 
else redis.call('del', KEYS[1]); 
     redis.call('publish', KEYS[2], ARGV[1]); 
     return 1;
     end;
return nil;
```

执行lock.unlock（）释放，每次对锁的次数-1直到发送锁次数为0执行del myLock 删除锁

##### watch dog自动延期机制

**客户端1加锁时间默认为30s，若30s后客户端1还未操作完成怎么办？**

redission启动watch dog机制，一旦加锁成功就会开启一个后台线程watch dog每隔10s检测一次 若锁仍然存在则续期（默认是30s）

##### redission缺点

最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。但是这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。接着就会导致，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，而客户端1也以为自己成功加了锁。此时就会导致多个客户端对一个分布式锁完成了加锁。这时系统在业务上一定会出现问题，导致脏数据的产生。所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。

**不管redis的部署时单机、主从、哨兵、集群都会有这种问题，因为主从同步是异步的，redlock解决这个问题**

#### redission中的锁

[http://ifeve.com/%E6%85%A2%E8%B0%88-redis-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-%E4%BB%A5%E5%8F%8A-redisson-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/](http://ifeve.com/慢谈-redis-实现分布式锁-以及-redisson-源码解析/)

##### 公平锁

多个客户端同时获取锁时，排队，当锁释放时，优先分配给先入队的客户端。

RLock fairLock=redisson.getFairLock("anyLock");

lock.lockAsync()异步加锁

![Redisson分布式锁原理（公平锁）](http://www.manongjc.com/images/csdn/15458866261545p88ms66u26.png)

**原理：**

**为什么有了有序集合还要用队列？**

虽然它们都有存放客户端，但是应该公平锁有个while死循环首先遍历队列判断元素情况，如果是有序集合无法方便判断，有序集合存放时间戳（尝试获取锁的时间）

**while死循环意义？**

对于维持的加锁排队队列来说，其中通过每次的循环判断会去处理已经超时的元素，这样就可以保证在可能的网络延迟或者客户端故障长时间没有释放锁导致的队列元素堆积不会发生，即即时清楚已经过期的队列元素。

**如果客户端A一直没有释放锁而客户端B超时时间已经大于当前时间？**

更新时间：客户端获取锁失败，会更新超时时间=锁过期时间+当前时间戳

**如果客户端B挂了 客户端C？**

过期机制：获取锁时首先检查队列首个客户端是否过期，如果过期则移除相关数据

客户端B挂了没有即时更新过期时间，则被清除数据 客户端C上位

```
SCORE(B)=当前时间戳+锁剩余时间+固定时间threadWaitTime(5s)
SCORE(C)=SCORE(B)+5S
SCORE(D)=SOCRE(B)+5S
```

**缓冲时间threadWaitTime意义？**

如果锁释放了，同一时刻，释放的时间和B尝试获取时间很接近（相同），下一个尝试获取锁的请求，有可能会把当前队列首位的threadId也认为是过期而去掉（没有更新过期时间）。所以，要加一个`threadWaitTime`(在Redisson中默认是5s)来缓冲。

**隐患**

```
当B获取到锁后,E进来了
SCORE(C)=SOCRE(B)+5S
SCORE(D)=SOCRE(B)+5S
SCORE(E)=SOCRE(C)+5S=SOCRE(B)+5S+5S//后面会越来越大 理论上没有问题
但若此时E挂了 A释放锁 必须等待SCORE(B)+5S+5S时间 E才会被去掉 F才能获取锁 当时间大时有问题

解决：Redisson引入了一个机制，就是在调用tryLock(1, 30, TimeUnit.Seconds)没有获取到锁的时候，检查是否为队列首，如果是，则队列中每个线程的在timeoutSet中的SCORE都减去threadWaitTime
```

**释放：**当第一个客户端释放锁时，通过while逻辑检测队列第一个元素是否满足已经过期，未过期则执行加锁操作，删除队列及有序集合中数据。

**支持可重入机制：**累计锁次数即可。

##### 联合锁

RedissonMultiLock对象可以将多个RLock对象关联为一个联锁，每个RLock对象实例可以来自于不同的Redisson实例。申请多个小锁联合成大锁，只有多个小锁在规定时间内加锁成功才算加锁成功。释放大锁就一次释放小锁即可。

```

RLock lock1 = redissonInstance1.getLock("lock1");
RLock lock2 = redissonInstance2.getLock("lock2");
RLock lock3 = redissonInstance3.getLock("lock3");
 
RedissonMultiLock lock = new RedissonMultiLock(lock1, lock2, lock3);
// 同时加锁：lock1 lock2 lock3
// 所有的锁都上锁成功才算成功。
lock.lock();
...
lock.unlock()

```

使用List队列存放需要获取的小锁

```
首先计算基本等待时间baseWaitTime=小锁个数*1500ms 假设有3个小锁
接着各个小锁尝试获取锁，一旦4500ms内没有获取到小锁。
获取小锁失败，会先判断当前需要获取锁的size-成功加锁的size==加锁失败次数，如果相等就break（说明没有获取锁失败过，接着下次获取小锁）；否则判断failedLocksLimit限定加小锁失败次数是否为0，为0则释放锁及相应复原操作，不为0则failedLocksLimit--
failedLocksLimit限定加小锁失败次数，只有==0才继续执行获取锁，否则直接返回false
假如小锁获取成功，则加入小锁队列
最后判断当前时间-开始加锁时间和baseWaitTime比较，超过了就释放所有小锁重新获取，否则加锁成功
```

##### 红锁Red Lock（基于多个Redis节点master 其它锁实现都是基于单个Redis节点）

继承于联合锁，每次获取锁，不是只对其中一台master进行获取锁操作，而是每次获取锁都要在集群大部分master【集群master数量n/2+1台实例】上成功获取到锁，并且要在规定的时间内获取到锁。

**基于单Redis节点的分布式锁**

```
SET resource_name my_random_value NX PX 30000
为什么设置随机字符串？
保证客户端释放的锁是自己拥有的那个锁。

假如获取锁时SET的不是一个随机字符串，而是一个固定值，那么可能会发生下面的执行序列：
客户端1获取锁成功。
客户端1在某个操作上阻塞了很长时间。
过期时间到了，锁自动释放了。
客户端2获取到了对应同一个资源的锁。
客户端1从阻塞中恢复过来，释放掉了客户端2持有的锁
```

**单机锁的问题？**

1. 客户端1从Master获取了锁。

2. Master宕机了，存储锁的key还没有来得及同步到Slave上。

3. Slave升级为Master。

4. 客户端2从新的Master获取到了对应同一个资源的锁

   于是客户端1和2共享同一个锁

   **红锁的出现 基于多个节点的锁**

   假如某个Master挂了，Slave上来后没有保存之前的key,因为redlock要获取大部分Master成功才算成功，所以即时少数Master挂了也不会影响到，除非同时大量都挂了。

```
获取红锁执行步骤：
1.获取当前时间（毫秒数）。
2.按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串my_random_value，也包含过期时间(比如PX 30000，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。
3.计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（>= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
4.如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。
5.如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的Redis Lua脚本）。

释放红锁：客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。
```

**红锁失效主要三种情况：**

- 时钟发生跳跃。
- 长时间的GC pause。
- 长时间的网络延迟。

**问题1：节点重启**

假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：

1. 客户端1成功锁住了A, B, C，**获取锁**成功（但D和E没有锁住）。
2. 节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。
3. 节点C重启后，客户端2锁住了C, D, E，**获取锁**成功。

这样，客户端1和客户端2同时获得了锁（针对同一资源）。

**对策**

在默认情况下，Redis的AOF持久化方式是每秒写一次磁盘（即执行fsync），因此最坏情况下可能丢失1秒的数据。为了尽可能不丢数据，Redis允许设置成每次修改数据都进行fsync，但这会降低性能。当然，即使执行了fsync也仍然有可能丢失数据（这取决于系统而不是Redis的实现）。

所以，上面分析的由于节点重启引发的锁失效问题，总是有可能出现的。为了应对这一问题，antirez又提出了**延迟重启**(delayed restarts)的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间(lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

关于Redlock还有一点细节值得拿出来分析一下：在最后**释放锁**的时候，antirez在算法描述中特别强调，客户端应该向所有Redis节点发起**释放锁**的操作。也就是说，即使当时向某个节点获取锁没有成功，在释放锁的时候也不应该漏掉这个节点。这是为什么呢？设想这样一种情况，客户端发给某个Redis节点的**获取锁**的请求成功到达了该Redis节点，这个节点也成功执行了`SET`操作，但是它返回给客户端的响应包却丢失了。这在客户端看来，获取锁的请求由于超时而失败了，但在Redis这边看来，加锁已经成功了。因此，释放锁的时候，客户端也应该对当时获取锁失败的那些Redis节点同样发起请求。实际上，这种情况在异步通信模型中是有可能发生的：客户端向服务器通信是正常的，但反方向却是有问题的。

**问题2：客户端长期堵塞？**

![分布式锁失效的时序](http://zhangtielei.com/assets/photos_redlock/unsafe-lock.png)

在上面的时序图中，假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。上图中出现的lease这个词可以暂且认为就等同于一个带有自动过期功能的锁。

客户端1在获得锁之后发生了很长时间的GC pause（锁时间根本没有增加，即自己不知道锁已经过期，watch dog没有触发），在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。

初看上去，有人可能会说，既然客户端1从GC pause中恢复过来以后不知道自己持有的锁已经过期了，那么它可以在访问共享资源之前先判断一下锁是否过期。但仔细想想，这丝毫也没有帮助。因为GC pause可能发生在任意时刻，也许恰好在判断完之后。

也有人会说，如果客户端使用没有GC的语言来实现，是不是就没有这个问题呢？Martin指出，系统环境太复杂，仍然有很多原因导致进程的pause，比如虚存造成的缺页故障(page fault)，再比如CPU资源的竞争。即使不考虑进程pause的情况，网络延迟也仍然会造成类似的结果。

总结起来就是说，即使锁服务本身是没有问题的，而仅仅是客户端有长时间的pause或网络延迟，仍然会造成两个客户端同时访问共享资源的冲突情况发生。而这种情况其实就是我们在前面已经提出来的“客户端长期阻塞导致锁过期”的那个疑问。

**对策**

![带有fencing token的时序](http://zhangtielei.com/assets/photos_redlock/fencing-tokens.png)

Martin给出了一种方法，称为fencing token。fencing token是一个单调递增的数字，当客户端成功获取锁的时候它随同锁一起返回给客户端。而客户端访问共享资源的时候带着这个fencing token，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突）。

在上图中，客户端1先获取到的锁，因此有一个较小的fencing token，等于33，而客户端2后获取到的锁，有一个较大的fencing token，等于34。客户端1从GC pause中恢复过来之后，依然是向存储服务发送访问请求，但是带了fencing token = 33。存储服务发现它之前已经处理过34的请求，所以会拒绝掉这次33的请求。这样就避免了冲突。（即拒绝小于当前token的请求）

**实际上怎么实现fencing token?**

如果通过fencing token已经可以实现分布式锁，那么还要red lock干嘛？

个人感觉实现fencing token过于累赘不合适。

**问题3：RedLock对系统时钟过于依赖**

1. 客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。

2. 节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。

3. 客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。

4. 客户端1和客户端2现在都认为自己持有了锁。

   **好的分布式算法**：

   应该基于异步模型(asynchronous model)，算法的安全性不应该依赖于任何记时假设(timing assumption)。在异步模型中：进程可能pause任意长的时间，消息可能在网络中延迟任意长的时间，甚至丢失，系统时钟也可能以任意方式出错。

   即使在非常极端的情况下（比如系统时钟严重错误），算法顶多是不能在有限的时间内给出结果而已，而不应该给出错误的结果。

   **决策**

   理论上一旦发生是没有好的对策，但可以防止。

   - 手动修改时钟这种人为原因，不要那么做就是了。否则的话，如果有人手动修改Raft协议的持久化日志，那么就算是Raft协议它也没法正常工作了。
   - 使用一个不会进行“跳跃”式调整系统时钟的ntpd程序（可能是通过恰当的配置），对于时钟的修改通过多次微小的调整来完成。

   **客户端获取的锁其实已经过期（网络延迟和程序停顿）客户端和锁服务器的消息延迟？**

   1. 客户端1向Redis节点A, B, C, D, E发起锁请求。
   2. 各个Redis节点已经把请求结果返回给了客户端1，但客户端1在收到请求结果之前进入了长时间的GC pause。
   3. 在所有的Redis节点上，锁过期了。
   4. 客户端2在A, B, C, D, E上获取到了锁。
   5. 客户端1从GC pause从恢复，收到了前面第2步来自各个Redis节点的请求结果。客户端1认为自己成功获取到了锁。
   6. 客户端1和客户端2现在都认为自己持有了锁。

**反驳：**客户端接受获取锁成功请求时，会计算获取锁过程的时间是否已经超过锁的有效时间，即可解决上述问题。

**客户端和访问资源服务器的延迟问题**：所有分布式都存在的问题，没什么好的解决办法

**总结**

**为了效率：**协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的email。

Redis单节点锁即可 RedLock实现过重。

**为了正确性(correctness)：**在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致(inconsistency)，数据丢失，文件损坏，或者其它严重的问题。

不应该使用RedLock，应该使用Zookeeper或支持事务的数据库

**只有单允许可能出现错误且集群才用 RedLock(一半一半)**

##### 读写锁

该对象允许同时有多个读取锁，但是最多只能有一个写锁。

写锁是排它锁，获取写锁的时候不能有已经获取读锁和写锁的，获取写锁后，除了本线程以外没发获取读写锁。

https://blog.csdn.net/zhxdick/article/details/82693646 

```
RReadWriteLock rwlock=redission.getLock("testLock");
rwlock.readLock().lock();//读锁
rwlock.writeLock().lock();//写锁
```

<img src="https://note.youdao.com/yws/api/personal/file/002A35274EEB49288DA90048C466B7D6?method=download&amp;shareKey=fa88cfaacadd2362abba8909a08daeba" alt="image" style="zoom: 50%;" />

```
加锁执行流程：同一客户端可以多次加读锁/写锁
第一次执行：
判断是加读还是写锁，mode中设置read/write 添加锁
【读读】
已经加了读锁情况下，若是同个客户端则+1 否则拼接多一个客户端线程
【读写】
发现mode为read，写锁直接失败
【写读】
1.发现mode为write且获取写锁的不为自己则获取失败
2.发现mode为write且获取写锁的为自己则获取成功
【写写】
获取写锁的为自己则获取成功，进行自增、延时等操作
获取写锁的不为自己则获取失败，返回剩余时间

释放锁：
【读锁释放】只有全部锁释放了，才是真的释放成功
首先判断当前客户端是否存在读锁，不存在返回null，存在则数量值-1，判断值是否为0，为0直接删除相关数据，不为0则刷新过期时间，返回false。接着计算hash中的key数量，如果大于1说明除了model还有其它key占领该读锁，遍历每个key获取它们的剩余时间，拿到当中最大的过期时间maxRemainTime，
如果大于0，则说明未过期刷新设置锁的过期时间maxRemainTime，这时表明锁释放不成功。如果mode==write直接返回false。
如果小于0，直接删除锁key，释放锁成功
【写锁释放】
首先判断当前客户端是否存在写锁，不存在返回null，存在则数量值-1，判断值是否为0，为0直接删除相关数据，不为0则刷新过期时间，返回false。接着计算hash中的key数量，如果等于1说明只有model了直接释放锁，
不为1，肯定是本线程还获取了读锁，将model改为read
```

**除了锁，redission还支持分布式object set list等操作**

### Zookeeper

**原理：**

- 客户端尝试创建一个znode节点，比如`/lock`。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode已存在），获取锁失败。

- 持有锁的客户端访问共享资源完成后，将znode删掉，这样其它客户端接下来就能来获取锁了。

- znode应该被创建成ephemeral的。这是znode的一个特性，它保证如果创建znode的那个客户端崩溃了，那么相应的znode会被自动删除。这保证了锁一定会被释放。

  zookeeper没有Redlock过期时间问题，而且能让锁自动释放

  **ZooKeeper是怎么检测出某个客户端已经崩溃了呢？**

  实际上，每个客户端都与ZooKeeper的某台服务器维护着一个Session，这个Session依赖定期的心跳(heartbeat)来维持。如果ZooKeeper长时间收不到客户端的心跳（这个时间称为Sesion的过期时间），那么它就认为Session过期了，通过这个Session所创建的所有的ephemeral类型的znode节点都会被自动删除。

**进程延迟GC pause问题？**

1. 客户端1创建了znode节点`/lock`，获得了锁。

2. 客户端1进入了长时间的GC pause。

3. 客户端1连接到ZooKeeper的Session过期了。znode节点`/lock`被自动删除。

4. 客户端2创建了znode节点`/lock`，从而获得了锁。

5. 客户端1从GC pause中恢复过来，它仍然认为自己持有锁。

   最后客户端1和2都认为自己拥有锁。

### 数据库分布式锁

#### 基于表记录

创建一张锁表，想要获取锁时，在表中增加一条记录，想要释放锁时，就删除相应的记录。

```
CREATE TABLE `database_lock` (
	`id` BIGINT NOT NULL AUTO_INCREMENT,
	`resource` int NOT NULL COMMENT '锁定的资源',
	`description` varchar(1024) NOT NULL DEFAULT "" COMMENT '描述',
	PRIMARY KEY (`id`),
	UNIQUE KEY `uiq_idx_resource` (`resource`) //添加唯一约束，防止多个操作。
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='数据库分布式锁表';


获取锁
INSERT INTO database_lock(resource, description) VALUES (1, 'lock');

释放锁
DELETE FROM database_lock WHERE resource=1;

```

**注意**

1.这种锁没有失效时间，一旦释放锁的操作失败就会导致锁记录一直在数据库中，其它线程无法获得锁。这个缺陷也很好解决，比如可以做一个定时任务去定时清理。
2.这种锁的可靠性依赖于数据库。建议设置备库，避免单点，进一步提高可靠性。
3.这种锁是非阻塞的，因为插入数据失败之后会直接报错，想要获得锁就需要再次操作。如果需要阻塞式的，可以弄个for循环、while循环之类的，直至INSERT成功再返回。
4.这种锁也是非可重入的，因为同一个线程在没有释放锁之前无法再次获得锁，因为数据库中已经存在同一份记录了。

想要实现可重入锁，可以在数据库中添加一些字段，比如获得锁的主机信息、线程信息等，那么在再次获得锁的时候可以先查询数据，如果当前的主机信息和线程信息等能被查到的话，可以直接把锁分配给它。

#### 乐观锁

添加version（版本号）

**id 表示主键 resource表示资源，不同的资源对应不同的主键**

STEP1 - 获取资源： SELECT resource, version FROM optimistic_lock WHERE id = 1
STEP2 - 执行业务逻辑
STEP3 - 更新资源：UPDATE optimistic_lock SET resource = resource -1, version = version + 1 WHERE id = 1 AND version = oldVersion

读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数 据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。

**注意**

乐观锁的优点比较明显，由于在检测数据冲突时并不依赖数据库本身的锁机制，不会影响请求的性能，当产生并发且并发量较小的时候只有少部分请求会失败。

**缺点**：是需要对表的设计增加额外的字段，增加了数据库的冗余，另外，当应用并发量高的时候，version值在频繁变化，则会导致大量请求失败，影响系统的可用性。

我们通过上述sql语句还可以看到，数据库锁都是作用于同一行数据记录上，这就导致一个明显的缺点，在一些特殊场景，如大促、秒杀等活动开展的时候，大量的请求同时请求同一条记录的行锁，会对数据库产生很大的写压力。所以综合数据库乐观锁的优缺点，乐观锁比较适合并发量不高，并且写操作不频繁的场景。

#### 悲观锁

通过数据库自带悲观锁  for update。必须指定索引，否则会锁表。

**首先要关闭Mysql的自动提交属性 set autocommit=0**

- STEP1 - 获取锁：SELECT * FROM database_lock WHERE id = 1 FOR UPDATE;。
- STEP2 - 执行业务逻辑。
- STEP3 - 释放锁：COMMIT。

**线程B在等待线程A释放锁之前执行STEP1，会堵塞，直到A释放锁。如果A长时间不释放锁，B会报错（只会堵塞一段时间）**

**注意**

在悲观锁中，每一次行数据的访问都是独占的，只有当正在访问该行数据的请求事务提交以后，其他请求才能依次访问该数据，否则将阻塞等待锁的获取。

悲观锁可以严格保证数据访问的安全。但是缺点也明显，即每次请求都会额外产生加锁的开销且未获取到锁的请求将会阻塞等待锁的获取，在高并发环境下，容易造成大量请求阻塞，影响系统可用性。另外，悲观锁使用不当还可能产生死锁的情况

#### 数据库中的死锁

##### 事务之间对资源的访问顺序

**出现原因：** 
一个用户A 访问表A（锁住了表A），然后又访问表B；另一个用户B 访问表B（锁住了表B），然后企图访问表A。

这时用户A由于用户B已经锁住表B，它必须等待用户B释放表B才能继续，同样用户B要等用户A释放表A才能继续，这就死锁就产生了。

**解决方法：** 
对于数据库的多表操作时，尽量按照相同的顺序进行处理，尽量避免同时锁定两个资源，如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。

##### 并发修改同一记录

**出现原因：**主要是由于没有一次性申请够权限的锁导致的。

用户A查询一条纪录，然后打算修改该条纪录时，用户B修改该条纪录。

这时用户A的共享锁打算上升到排他锁，但B占有排他锁却堵塞住等待A释放共享锁，故形成死锁。

**解决方法：**

a. 乐观锁，实现写-写并发

b. 悲观锁：使用悲观锁进行控制。悲观锁大多数情况下依靠数据库的锁机制实现，如Oracle的Select … for update语句，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。

##### 过多的表锁

如果在事务中执行了一条不满足条件的update语句，则执行全表扫描，把行级锁上升为表级锁，多个这样的事务执行后，就很容易产生死锁和阻塞。类似的情 况还有当表中的数据量非常庞大而索引建的过少或不合适的时候，使得经常发生全表扫描，最终应用系统会越来越慢，最终发生阻塞或死锁。

**解决方法：**

SQL语句中不要使用太复杂的关联多表的查询；

使用“执行计划”对SQL语句进行分析，对于有全表扫描的SQL语句，建立相应的索引进行优化。

##### 总结

1）以固定的顺序访问表和行。即按顺序申请锁，这样就不会造成互相等待的场面。

2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。

3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。

4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。

5）为表添加合理的索引。如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。